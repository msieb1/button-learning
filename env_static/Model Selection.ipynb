{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from torch_utils import set_gpu_mode, get_numpy, from_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_gpu_mode(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/1/data.npz')\n",
    "colors = data.get('colors')\n",
    "shapes = data.get('shapes')\n",
    "\n",
    "colors_ohe = OneHotEncoder(categories='auto').fit_transform(colors.reshape(-1, 1)).toarray()\n",
    "shapes_ohe = OneHotEncoder(categories='auto').fit_transform(shapes.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[colors_ohe, shapes_ohe]\n",
    "y = data.get('is_buttons').reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_weight_ratio = 1 / np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_t, y_tr, y_t = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ds_tr = TensorDataset(from_numpy(X_tr), from_numpy(y_tr))\n",
    "ds_t = TensorDataset(from_numpy(X_t), from_numpy(y_t))\n",
    "\n",
    "weights_tr = np.ones(len(y_tr)) \n",
    "weights_tr[y_tr.flatten() == 1] = pos_neg_weight_ratio\n",
    "weights_t = np.ones(len(y_t))\n",
    "weights_t[y_t.flatten() == 1] = pos_neg_weight_ratio\n",
    "\n",
    "dl_tr = DataLoader(ds_tr, batch_size=16, sampler=WeightedRandomSampler(weights_tr, len(weights_tr)))\n",
    "dl_t = DataLoader(ds_t, batch_size=16, sampler=WeightedRandomSampler(weights_t, len(weights_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x, requires_grad=False, volatile=False):\n",
    "    \"\"\"\n",
    "    Varialbe type that automatically choose cpu or cuda\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, requires_grad=requires_grad, volatile=volatile)\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MaskedLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.mask_flag = False\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        self.mask = to_var(mask, requires_grad=False)\n",
    "        self.weight.data = self.weight.data*self.mask.data\n",
    "        self.mask_flag = True\n",
    "    \n",
    "    def get_mask(self):\n",
    "        return self.mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.mask_flag == True:\n",
    "            weight = self.weight*self.mask\n",
    "            return F.linear(x, weight, self.bias)\n",
    "        else:\n",
    "            return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "class SEM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, fcs=[]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fcs = nn.ModuleList()\n",
    "        for i, fc_size in enumerate(fcs):\n",
    "            self.fcs.append(MaskedLinear(input_size if i == 0 else fcs[i-1], fc_size))\n",
    "            \n",
    "        self.fcs.append(MaskedLinear(input_size if len(fcs) == 0 else fcs[-1], output_size))\n",
    "        \n",
    "    def apply_layer_mask(self, layer, n):\n",
    "        fc = self.fcs[layer]\n",
    "        if not fc.mask_flag:\n",
    "            fc.set_mask(torch.ones(fc.weight.shape))\n",
    "        mask = fc.get_mask()\n",
    "        \n",
    "        mask_np_1d = get_numpy(mask).flatten()\n",
    "        w = get_numpy(fc.weight)\n",
    "        w_abs_1d = np.abs(w.flatten())\n",
    "        w_abs_1d[mask_np_1d == 0] = np.inf\n",
    "        \n",
    "        idx_1d = w_abs_1d.argsort()[:n]\n",
    "        x_idx, y_idx = np.unravel_index(idx_1d, w.shape)\n",
    "        \n",
    "        mask[x_idx, y_idx] = 0\n",
    "        \n",
    "        fc.set_mask(mask)\n",
    "        \n",
    "#         print(mask)\n",
    "#         print(w)\n",
    "            \n",
    "        print('Pruned {}/{} weights'.format(np.sum(1 - get_numpy(mask)), np.prod(mask.shape)))\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        for fc in self.fcs[:-1]:\n",
    "            x = fc(x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = self.fcs[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dl, opt, criterion):\n",
    "    for batch in dl:\n",
    "        opt.zero_grad()\n",
    "        loss = criterion(model(batch[0]), batch[1])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "def acc(pred, gt):\n",
    "    pred, gt = get_numpy(pred), get_numpy(gt)\n",
    "    pred_labels = pred > 0\n",
    "\n",
    "    return np.mean(pred_labels == gt)\n",
    "\n",
    "def eval_loss_acc(model, dl, criterion, acc):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    for batch in dl:\n",
    "        pred = model(batch[0])\n",
    "        loss = criterion(pred, batch[1])\n",
    "        total_loss += get_numpy(loss)\n",
    "        total_acc += acc(pred, batch[1])\n",
    "\n",
    "    return total_loss / len(dl), total_acc / len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 weight 0.5\n"
     ]
    }
   ],
   "source": [
    "model = SEM(X_tr.shape[1], 1, fcs=[]).cuda()\n",
    "model_weights = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' not in name:\n",
    "        model_weights.append(param)\n",
    "        \n",
    "for fc in model.fcs:\n",
    "    fc.bias.requires_grad = False\n",
    "        \n",
    "opt = torch.optim.Adam(model_weights, lr=0.0001)\n",
    "\n",
    "def init_bias(m):\n",
    "    if type(m) == MaskedLinear:\n",
    "        m.bias.data.fill_(-0.1)\n",
    "model.apply(init_bias)\n",
    "\n",
    "def get_criterion(l1_weight, params):\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    l1_crit = nn.L1Loss(reduction='sum')\n",
    "    \n",
    "    def criterion(X, y):\n",
    "        bce_loss = bce(X, y)\n",
    "        l1_loss = 0\n",
    "        for param in params:\n",
    "            l1_loss += l1_crit(param, torch.zeros(param.shape).cuda())\n",
    "        \n",
    "        return bce_loss + l1_weight * l1_loss\n",
    "\n",
    "    return criterion\n",
    "        \n",
    "l1_weight = 0.5 # log(len(X_tr))\n",
    "print('l1 weight', l1_weight)\n",
    "criterion = get_criterion(l1_weight, model_weights)\n",
    "# criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20 | Loss tr 1.147 t 1.157 | Acc tr 0.90 t 0.94\n",
      "2/20 | Loss tr 1.130 t 1.137 | Acc tr 0.94 t 0.80\n",
      "3/20 | Loss tr 1.135 t 1.141 | Acc tr 0.95 t 0.86\n",
      "4/20 | Loss tr 1.140 t 1.154 | Acc tr 0.89 t 0.90\n",
      "5/20 | Loss tr 1.129 t 1.143 | Acc tr 0.91 t 0.93\n",
      "6/20 | Loss tr 1.128 t 1.162 | Acc tr 0.96 t 0.87\n",
      "7/20 | Loss tr 1.128 t 1.129 | Acc tr 0.90 t 0.90\n",
      "8/20 | Loss tr 1.130 t 1.137 | Acc tr 0.95 t 0.94\n",
      "9/20 | Loss tr 1.119 t 1.147 | Acc tr 0.94 t 0.80\n",
      "10/20 | Loss tr 1.122 t 1.150 | Acc tr 0.94 t 0.83\n",
      "11/20 | Loss tr 1.132 t 1.146 | Acc tr 0.90 t 0.87\n",
      "12/20 | Loss tr 1.129 t 1.142 | Acc tr 0.90 t 0.90\n",
      "13/20 | Loss tr 1.128 t 1.101 | Acc tr 0.96 t 0.97\n",
      "14/20 | Loss tr 1.139 t 1.136 | Acc tr 0.85 t 0.90\n",
      "15/20 | Loss tr 1.126 t 1.125 | Acc tr 0.88 t 0.97\n",
      "16/20 | Loss tr 1.124 t 1.177 | Acc tr 0.88 t 0.77\n",
      "17/20 | Loss tr 1.117 t 1.113 | Acc tr 0.94 t 1.00\n",
      "18/20 | Loss tr 1.116 t 1.125 | Acc tr 0.92 t 0.90\n",
      "19/20 | Loss tr 1.098 t 1.118 | Acc tr 0.99 t 0.96\n",
      "20/20 | Loss tr 1.107 t 1.109 | Acc tr 0.93 t 0.90\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "loss_trs, acc_trs, loss_ts, acc_ts = [], [], [], []\n",
    "for e in range(epochs):\n",
    "    train(model, dl_tr, opt, criterion)\n",
    "    \n",
    "    loss_tr, acc_tr = eval_loss_acc(model, dl_tr, criterion, acc)\n",
    "    loss_t, acc_t = eval_loss_acc(model, dl_t, criterion, acc)\n",
    "    \n",
    "    loss_trs.append(loss_tr)\n",
    "    acc_trs.append(acc_tr)\n",
    "    loss_ts.append(loss_t)\n",
    "    acc_ts.append(acc_t)\n",
    "    \n",
    "    print('{}/{} | Loss tr {:.3f} t {:.3f} | Acc tr {:.2f} t {:.2f}'.format(e+1, epochs, loss_tr, loss_t, acc_tr, acc_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcs.0.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1681, -0.0860, -0.2387,  0.3097,  0.0221, -0.2845]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fcs.0.bias\n",
      "Parameter containing:\n",
      "tensor([-0.1000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned 6.0/6 weights\n"
     ]
    }
   ],
   "source": [
    "model.apply_layer_mask(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 | Loss tr 0.064 t 0.048 | Acc tr 1.00 t 1.00\n",
      "2/5 | Loss tr 0.053 t 0.034 | Acc tr 1.00 t 1.00\n",
      "3/5 | Loss tr 0.042 t 0.034 | Acc tr 1.00 t 1.00\n",
      "4/5 | Loss tr 0.030 t 0.037 | Acc tr 1.00 t 1.00\n",
      "5/5 | Loss tr 0.032 t 0.023 | Acc tr 1.00 t 1.00\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "loss_trs, acc_trs, loss_ts, acc_ts = [], [], [], []\n",
    "for e in range(epochs):\n",
    "    train(model, dl_tr, opt, criterion)\n",
    "    \n",
    "    loss_tr, acc_tr = eval_loss_acc(model, dl_tr, criterion, acc)\n",
    "    loss_t, acc_t = eval_loss_acc(model, dl_t, criterion, acc)\n",
    "    \n",
    "    loss_trs.append(loss_tr)\n",
    "    acc_trs.append(acc_tr)\n",
    "    loss_ts.append(loss_t)\n",
    "    acc_ts.append(acc_t)\n",
    "    \n",
    "    print('{}/{} | Loss tr {:.3f} t {:.3f} | Acc tr {:.2f} t {:.2f}'.format(e+1, epochs, loss_tr, loss_t, acc_tr, acc_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned 15.0/60 weights\n"
     ]
    }
   ],
   "source": [
    "model.apply_mask(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 | Loss tr 0.346 t 0.384 | Acc tr 1.00 t 1.00\n",
      "2/5 | Loss tr 0.295 t 0.404 | Acc tr 1.00 t 1.00\n",
      "3/5 | Loss tr 0.265 t 0.255 | Acc tr 1.00 t 1.00\n",
      "4/5 | Loss tr 0.220 t 0.213 | Acc tr 1.00 t 1.00\n",
      "5/5 | Loss tr 0.177 t 0.162 | Acc tr 1.00 t 1.00\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "loss_trs, acc_trs, loss_ts, acc_ts = [], [], [], []\n",
    "for e in range(epochs):\n",
    "    train(model, dl_tr, opt, criterion)\n",
    "    \n",
    "    loss_tr, acc_tr = eval_loss_acc(model, dl_tr, criterion, acc)\n",
    "    loss_t, acc_t = eval_loss_acc(model, dl_t, criterion, acc)\n",
    "    \n",
    "    loss_trs.append(loss_tr)\n",
    "    acc_trs.append(acc_tr)\n",
    "    loss_ts.append(loss_t)\n",
    "    acc_ts.append(acc_t)\n",
    "    \n",
    "    print('{}/{} | Loss tr {:.3f} t {:.3f} | Acc tr {:.2f} t {:.2f}'.format(e+1, epochs, loss_tr, loss_t, acc_tr, acc_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned 14.0/60 weights\n"
     ]
    }
   ],
   "source": [
    "model.apply_mask(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SK Learn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35578257, -0.65637182, -0.58970924,  0.99510783, -1.01403316,\n",
       "        -1.00012471]])"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_model = LogisticRegression(C=3e-1, penalty='l1', solver='liblinear', class_weight='balanced')\n",
    "lreg_model.fit(X_tr, y_tr.ravel())\n",
    "lreg_model.coef_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.73168457e-05])"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_model.coef_ = np.array([1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-567-a29259b9a73f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_tr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_t_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "y_tr_pred = lreg_model.predict(X_tr)\n",
    "print(np.mean(y_tr_pred == y_tr.flatten()))\n",
    "\n",
    "y_t_pred = lreg_model.predict(X_t)\n",
    "print(np.mean(y_t_pred == y_t.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (jacky)",
   "language": "python",
   "name": "jacky"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
